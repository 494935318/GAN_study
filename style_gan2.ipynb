{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-24db55f4f856>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers \n",
    "import time\n",
    "import dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mappingnetwork(keras.layers.Layer):\n",
    "    def __init__(self,outdim,**key):\n",
    "        super(Mappingnetwork,self).__init__(**key)\n",
    "        self.out_dim=outdim\n",
    "    def build(self,input_shape):\n",
    "        \n",
    "        self.norm=[layers.BatchNormalization() for _ in range(9)]\n",
    "        self.layers=[layers.Dense(self.out_dim)for i in range (8)]\n",
    "        self.act=layers.LeakyReLU(0.2)\n",
    "    def call(self,inputs,training=True):\n",
    "        out=self.norm[0](inputs,training=training)\n",
    "        for i in range(4):\n",
    "            out1=self.layers[i*2](out)\n",
    "            out2=self.act(out1)\n",
    "            out=self.norm[i*2+1](out,training=training)\n",
    "            out3=self.layers[i*2+1](out2)\n",
    "            out4=self.act(out3)\n",
    "            out4=self.norm[i*2+2](out4,training=training)\n",
    "            out=out4+out\n",
    "        return out \n",
    "    def get_config(self):\n",
    "        return {\"out_dim\":self.out_dim}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIn(layers.Layer):\n",
    "    def __init__(self,shape,**key):\n",
    "        super(AdaIn,self).__init__(**key)\n",
    "        self.shape=shape\n",
    "    def build(self,input_shape):\n",
    "        self.Dense=keras.layers.Dense(self.shape)\n",
    "        self.act=keras.layers.LeakyReLU(0.2)\n",
    "    def call(self,w,input):\n",
    "        [m,b]=tf.nn.moments(input,axes=[1,2],keepdims=True)\n",
    "        out1=input/b\n",
    "        b=self.Dense(w)\n",
    "        b=self.act(b)\n",
    "        b=tf.expand_dims(b,1)\n",
    "        b=tf.expand_dims(b,1)\n",
    "        out=out1*b\n",
    "        return out\n",
    "    def get_config(self):\n",
    "        return {\"shape\":self.shape}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sub_sublayer(layers.Layer):\n",
    "    def __init__(self,laten_dim):\n",
    "        super(sub_sublayer,self).__init__()\n",
    "        self.laten_dim=laten_dim\n",
    "    def build(self,input_shape):\n",
    "        self.adin=AdaIn(self.laten_dim)\n",
    "        self.conv=layers.Conv2D(self.laten_dim,2,padding='same')\n",
    "        self.act=layers.LeakyReLU(0.2)\n",
    "    def call(self,input,w,b):\n",
    "        out1=self.conv(input)\n",
    "        out1=self.act(out1)\n",
    "        b=tf.random.normal([tf.shape(w)[0],1,1,self.laten_dim])\n",
    "        out2=out1+0.001*b\n",
    "        out=self.adin(w,out2)\n",
    "        return out\n",
    "    def get_config(self):\n",
    "        return {\"laten_dim\":self.laten_dim}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class style_layer(layers.Layer):\n",
    "    def __init__(self,laten_dim,**key):\n",
    "        super(style_layer,self).__init__(**key)\n",
    "        self.laten_dim=laten_dim\n",
    "    def build(self,input_shape):\n",
    "        self.sub=[sub_sublayer(self.laten_dim) for i in range(2)]\n",
    "        self.upsambel=layers.Conv2DTranspose(self.laten_dim,4,2,padding=\"same\")\n",
    "        self.act=layers.LeakyReLU(0.2)\n",
    "    def call(self,input,w,b):\n",
    "        out1=self.upsambel(input)\n",
    "        out1=self.act(out1)\n",
    "        out2=self.sub[0](out1,w,b)\n",
    "        out3=self.sub[1](out2,w,b)\n",
    "        return out3\n",
    "    def get_config(self):\n",
    "        return {\"laten_dim\":self.laten_dim}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen(layers.Layer):\n",
    "    def __init__(self,laten_dim,**key):\n",
    "        super(gen,self).__init__(**key)\n",
    "        self.lat=laten_dim\n",
    "    def build(self,input_shape):\n",
    "        self.constant=tf.random.normal([1,4,3,self.lat])\n",
    "        self.sub_layer=[style_layer(self.lat) for _ in range(5)]\n",
    "        self.Dense=layers.Conv2D(3,3,activation=\"sigmoid\",padding=\"same\")\n",
    "    def call(self,w,b):\n",
    "        l1=tf.tile(self.constant,[tf.shape(w)[0],1,1,1])\n",
    "        for i in range(5):\n",
    "            l2=self.sub_layer[i](l1,w,b)\n",
    "            l3=layers.UpSampling2D(2,interpolation='bilinear')(l1)\n",
    "            l1=l2+l3\n",
    "        l1=self.Dense(l1)\n",
    "        return l1 \n",
    "    def get_config(self):\n",
    "        return {\"laten_dim\":self.lat}\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class down_layer(layers.Layer):\n",
    "    def __init__(self,laten_dim,**key):\n",
    "        super(down_layer,self).__init__(**key)\n",
    "        self.lat=laten_dim\n",
    "    def build(self,input_shape):\n",
    "        self.conv2=layers.Conv2D(self.lat,2,1,padding=\"same\")\n",
    "        self.down=layers.Conv2D(self.lat,4,2,padding=\"same\")\n",
    "        self.conv1=layers.Conv2D(self.lat,1,1,padding=\"same\")\n",
    "        self.act=layers.LeakyReLU(0.2)\n",
    "    def call(self,input):\n",
    "        l1=self.conv2(input)\n",
    "        l1=self.act(l1)\n",
    "        l2=self.down(l1)\n",
    "        l2=self.act(l2)\n",
    "        l3=self.conv1(l2)\n",
    "        l3=self.act(l3)\n",
    "        l4=layers.AveragePooling2D(2)(input)\n",
    "        return l3+l4\n",
    "    def get_config(self):\n",
    "        return {\"laten_dim\":self.lat}\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(keras.Model):\n",
    "    def __init__(self,laten_dim,**key):\n",
    "        super(discriminator,self).__init__(**key)\n",
    "        self.lat=laten_dim\n",
    "    def build(self,input_shape):\n",
    "        self.lays=[down_layer(self.lat) for i in range (5)]\n",
    "        self.Dense0=keras.layers.Conv2D(self.lat,1,1,padding=\"same\")\n",
    "        self.Dense1=keras.layers.Dense(64)\n",
    "        self.act=layers.LeakyReLU(0.2)\n",
    "        self.Dense2=keras.layers.Dense(32)\n",
    "        self.Dense3=keras.layers.Dense(1)\n",
    "    def call(self,input):\n",
    "        l1=self.Dense0(input)\n",
    "        l1=self.act(l1)\n",
    "        for i in range (5):\n",
    "            l1=self.lays[i](l1)\n",
    "        l2=layers.GlobalAveragePooling2D()(l1)\n",
    "        l2=self.Dense1(l2)\n",
    "        l2=self.act(l2)\n",
    "        l2=self.Dense2(l2)\n",
    "        l2=self.act(l2)\n",
    "        l2=self.Dense3(l2)\n",
    "        return l2\n",
    "    def get_config(self):\n",
    "        return {\"laten_dim\":self.lat}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=keras.Input([64,])\n",
    "Map1=Mappingnetwork(64)\n",
    "gen1=gen(64)\n",
    "l1=Map1(input)\n",
    "l2=gen1(l1,None)\n",
    "generator=keras.Model(input,l2)\n",
    "discriminator_=discriminator(128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate one optimizer for the discriminator and another for the generator.\n",
    "d_optimizer = keras.optimizers.RMSprop(learning_rate=0.0004)\n",
    "g_optimizer = keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "# Instantiate a loss function.\n",
    "# loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "latent_dim=64\n",
    "@tf.function\n",
    "def train_step(real_images):\n",
    "    # Sample random points in the latent space\n",
    "    for i in range(1):\n",
    "        \n",
    "        random_vector=tf.random.normal(shape=[real_images.shape[0],latent_dim])\n",
    "        with tf.GradientTape() as tape1:\n",
    "            with tf.GradientTape() as tape2: \n",
    "                generate_image_1=generator(random_vector)\n",
    "                alph=tf.random.normal(shape=[real_images.shape[0],1,1,1])\n",
    "                generate_image_2=alph*real_images+(1-alph)*generate_image_1\n",
    "                out_real=discriminator_(real_images)\n",
    "                out_gen_1=discriminator_(generate_image_1)\n",
    "                out_gen_2=discriminator_(generate_image_2)\n",
    "            grade_1=tape2.gradient(out_gen_2,generate_image_2)\n",
    "            grade_l2=gradien_panalty_loss(grade_1)\n",
    "            loss_dis=dis_loss(out_real,out_gen_1,grade_l2,20)\n",
    "        grade_2=tape1.gradient(loss_dis,discriminator_.trainable_variables)\n",
    "        d_optimizer.apply_gradients(zip(grade_2,discriminator_.trainable_variables))\n",
    "    for i in range(1):\n",
    "        random_vector=tf.random.normal(shape=[real_images.shape[0],latent_dim])\n",
    "        with tf.GradientTape() as tape:\n",
    "            generate_image= generator(random_vector)\n",
    "            out=discriminator_(generate_image)\n",
    "            loss_gen=gen_loss(out)\n",
    "        grade=tape.gradient(loss_gen,generator.trainable_variables)\n",
    "        g_optimizer.apply_gradients(zip(grade,generator.trainable_variables))\n",
    "    return loss_dis,loss_gen, generate_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def dis_loss(real,fake,grade,lamda):\n",
    "    loss=tf.reduce_mean(fake)-tf.reduce_mean(real)+lamda*grade\n",
    "    return loss\n",
    "@ tf.function\n",
    "def gen_loss(fake):\n",
    "    return -tf.reduce_mean(fake) \n",
    "@ tf.function\n",
    "def gradien_panalty_loss(grade):\n",
    "    grad_sqr=tf.square(grade)\n",
    "    grad_sqr_sum=tf.reduce_sum(grad_sqr,axis=np.arange(1,len(grad_sqr.shape)))\n",
    "    grad_l2_norm=tf.sqrt(grad_sqr_sum)\n",
    "    grad_penalty=tf.square(1-grad_l2_norm)\n",
    "    return tf.reduce_mean(grad_penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Prepare the dataset. We use both the training & test MNIST digits.\n",
    "batch_size = 32\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "# all_digits = np.concatenate([x_train, x_test])\n",
    "# all_label=keras.utils.to_categorical( np.concatenate([y_train,y_test]))\n",
    "# all_digits = all_digits.astype(\"float32\") / 255.0\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((all_digits,all_label))\n",
    "img_path = glob.glob(r'E:/CelebA/Img/img_align_celeba/img_align_celeba/*.jpg')\n",
    "dataset1, img_shape, _ = dataset.make_anime_dataset(img_path, batch_size,[128,96])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chekpoint=tf.train.Checkpoint(generator=generator,discriminator=discriminator_)\n",
    "manager=tf.train.CheckpointManager(chekpoint,'./face_dis2',max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chekpoint.restore(tf.train.latest_checkpoint('./face_dis2'))\n",
    "discriminator_=chekpoint.discriminator\n",
    "generator=chekpoint.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator_.build(input_shape=[batch_size,128,96])\n",
    "# generator.build(input_shape=[batch_size,128,96])\n",
    "# discriminator_.set_weights(discriminator_1.get_weights())\n",
    "# generator.set_weights(generator1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "oss at step 5600: -14.82\n",
      "adversarial loss at step 5600: -265.54\n",
      "discriminator loss at step 5800: -9.56\n",
      "adversarial loss at step 5800: -97.39\n",
      "discriminator loss at step 6000: -10.91\n",
      "adversarial loss at step 6000: 309.86\n",
      "discriminator loss at step 6200: 3.45\n",
      "adversarial loss at step 6200: 177.94\n",
      "./face_dis2\\ckpt-1\n",
      "\n",
      "Start epoch 1\n",
      "discriminator loss at step 0: -15.93\n",
      "adversarial loss at step 0: 83.68\n",
      "discriminator loss at step 200: -1.71\n",
      "adversarial loss at step 200: -152.53\n",
      "discriminator loss at step 400: -13.01\n",
      "adversarial loss at step 400: -2.05\n",
      "discriminator loss at step 600: -24.10\n",
      "adversarial loss at step 600: -144.45\n",
      "discriminator loss at step 800: -16.81\n",
      "adversarial loss at step 800: 114.23\n",
      "discriminator loss at step 1000: -27.52\n",
      "adversarial loss at step 1000: 68.78\n",
      "discriminator loss at step 1200: 10.28\n",
      "adversarial loss at step 1200: 3.54\n",
      "discriminator loss at step 1400: -24.35\n",
      "adversarial loss at step 1400: 37.93\n",
      "discriminator loss at step 1600: -9.35\n",
      "adversarial loss at step 1600: 46.04\n",
      "discriminator loss at step 1800: 2.12\n",
      "adversarial loss at step 1800: -132.63\n",
      "discriminator loss at step 2000: -19.86\n",
      "adversarial loss at step 2000: -190.40\n",
      "discriminator loss at step 2200: -15.49\n",
      "adversarial loss at step 2200: 17.44\n",
      "discriminator loss at step 2400: -5.29\n",
      "adversarial loss at step 2400: -187.71\n",
      "discriminator loss at step 2600: -10.94\n",
      "adversarial loss at step 2600: 60.65\n",
      "discriminator loss at step 2800: 43.66\n",
      "adversarial loss at step 2800: 97.49\n",
      "discriminator loss at step 3000: -6.25\n",
      "adversarial loss at step 3000: -10.26\n",
      "discriminator loss at step 3200: -5.93\n",
      "adversarial loss at step 3200: 204.92\n",
      "discriminator loss at step 3400: 2.60\n",
      "adversarial loss at step 3400: 57.53\n",
      "discriminator loss at step 3600: -14.11\n",
      "adversarial loss at step 3600: 38.37\n",
      "discriminator loss at step 3800: -5.19\n",
      "adversarial loss at step 3800: 15.42\n",
      "discriminator loss at step 4000: -19.61\n",
      "adversarial loss at step 4000: 74.29\n",
      "discriminator loss at step 4200: -3.66\n",
      "adversarial loss at step 4200: -316.28\n",
      "discriminator loss at step 4400: -7.17\n",
      "adversarial loss at step 4400: 133.34\n",
      "discriminator loss at step 4600: -6.87\n",
      "adversarial loss at step 4600: -333.16\n",
      "discriminator loss at step 4800: -21.22\n",
      "adversarial loss at step 4800: -5.30\n",
      "discriminator loss at step 5000: -13.74\n",
      "adversarial loss at step 5000: -22.97\n",
      "discriminator loss at step 5200: -7.05\n",
      "adversarial loss at step 5200: 112.63\n",
      "discriminator loss at step 5400: -1.79\n",
      "adversarial loss at step 5400: 13.63\n",
      "discriminator loss at step 5600: -0.53\n",
      "adversarial loss at step 5600: 466.99\n",
      "discriminator loss at step 5800: -7.53\n",
      "adversarial loss at step 5800: -93.95\n",
      "discriminator loss at step 6000: -32.48\n",
      "adversarial loss at step 6000: 56.77\n",
      "discriminator loss at step 6200: -10.80\n",
      "adversarial loss at step 6200: -130.70\n",
      "./face_dis2\\ckpt-2\n",
      "\n",
      "Start epoch 2\n",
      "discriminator loss at step 0: -14.03\n",
      "adversarial loss at step 0: 58.30\n",
      "discriminator loss at step 200: -7.84\n",
      "adversarial loss at step 200: -162.16\n",
      "discriminator loss at step 400: -7.57\n",
      "adversarial loss at step 400: 154.66\n",
      "discriminator loss at step 600: -0.49\n",
      "adversarial loss at step 600: -8.45\n",
      "discriminator loss at step 800: -5.42\n",
      "adversarial loss at step 800: -154.25\n",
      "discriminator loss at step 1000: -19.94\n",
      "adversarial loss at step 1000: -230.08\n",
      "discriminator loss at step 1200: 4.14\n",
      "adversarial loss at step 1200: -131.66\n",
      "discriminator loss at step 1400: -14.31\n",
      "adversarial loss at step 1400: 5.33\n",
      "discriminator loss at step 1600: -1.26\n",
      "adversarial loss at step 1600: -66.94\n",
      "discriminator loss at step 1800: -16.66\n",
      "adversarial loss at step 1800: 121.58\n",
      "discriminator loss at step 2000: -11.87\n",
      "adversarial loss at step 2000: 146.95\n",
      "discriminator loss at step 2200: -12.02\n",
      "adversarial loss at step 2200: 26.33\n",
      "discriminator loss at step 2400: -4.78\n",
      "adversarial loss at step 2400: 63.03\n",
      "discriminator loss at step 2600: -9.50\n",
      "adversarial loss at step 2600: 174.32\n",
      "discriminator loss at step 2800: -5.92\n",
      "adversarial loss at step 2800: 93.97\n",
      "discriminator loss at step 3000: -13.53\n",
      "adversarial loss at step 3000: 7.02\n",
      "discriminator loss at step 3200: -6.50\n",
      "adversarial loss at step 3200: -117.84\n",
      "discriminator loss at step 3400: 0.53\n",
      "adversarial loss at step 3400: 216.02\n",
      "discriminator loss at step 3600: -1.41\n",
      "adversarial loss at step 3600: -67.72\n",
      "discriminator loss at step 3800: -16.04\n",
      "adversarial loss at step 3800: 13.09\n",
      "discriminator loss at step 4000: 2.31\n",
      "adversarial loss at step 4000: 100.30\n",
      "discriminator loss at step 4200: -5.78\n",
      "adversarial loss at step 4200: 47.23\n",
      "discriminator loss at step 4400: -6.04\n",
      "adversarial loss at step 4400: -67.52\n",
      "discriminator loss at step 4600: -13.55\n",
      "adversarial loss at step 4600: 201.38\n",
      "discriminator loss at step 4800: -21.98\n",
      "adversarial loss at step 4800: -206.24\n",
      "discriminator loss at step 5000: -8.94\n",
      "adversarial loss at step 5000: 30.98\n",
      "discriminator loss at step 5200: -8.10\n",
      "adversarial loss at step 5200: -109.74\n",
      "discriminator loss at step 5400: 1.96\n",
      "adversarial loss at step 5400: -18.24\n",
      "discriminator loss at step 5600: -5.17\n",
      "adversarial loss at step 5600: -86.34\n",
      "discriminator loss at step 5800: -16.65\n",
      "adversarial loss at step 5800: 175.12\n",
      "discriminator loss at step 6000: -3.85\n",
      "adversarial loss at step 6000: 271.04\n",
      "discriminator loss at step 6200: 10.81\n",
      "adversarial loss at step 6200: -332.04\n",
      "./face_dis2\\ckpt-3\n",
      "\n",
      "Start epoch 3\n",
      "discriminator loss at step 0: -6.18\n",
      "adversarial loss at step 0: 78.10\n",
      "discriminator loss at step 200: 6.01\n",
      "adversarial loss at step 200: -88.05\n",
      "discriminator loss at step 400: 4.35\n",
      "adversarial loss at step 400: 126.72\n",
      "discriminator loss at step 600: -9.55\n",
      "adversarial loss at step 600: 125.53\n",
      "discriminator loss at step 800: -10.70\n",
      "adversarial loss at step 800: 109.86\n",
      "discriminator loss at step 1000: -15.61\n",
      "adversarial loss at step 1000: -36.55\n",
      "discriminator loss at step 1200: -28.85\n",
      "adversarial loss at step 1200: -194.67\n",
      "discriminator loss at step 1400: -15.19\n",
      "adversarial loss at step 1400: 116.24\n",
      "discriminator loss at step 1600: -18.10\n",
      "adversarial loss at step 1600: 173.94\n",
      "discriminator loss at step 1800: -11.05\n",
      "adversarial loss at step 1800: 166.33\n",
      "discriminator loss at step 2000: -11.20\n",
      "adversarial loss at step 2000: 162.35\n",
      "discriminator loss at step 2200: -6.46\n",
      "adversarial loss at step 2200: -128.49\n",
      "discriminator loss at step 2400: -16.01\n",
      "adversarial loss at step 2400: -165.94\n",
      "discriminator loss at step 2600: -6.78\n",
      "adversarial loss at step 2600: 119.59\n",
      "discriminator loss at step 2800: -19.62\n",
      "adversarial loss at step 2800: 7.45\n",
      "discriminator loss at step 3000: -6.57\n",
      "adversarial loss at step 3000: -0.81\n",
      "discriminator loss at step 3200: -7.87\n",
      "adversarial loss at step 3200: 275.72\n",
      "discriminator loss at step 3400: 12.52\n",
      "adversarial loss at step 3400: -22.50\n",
      "discriminator loss at step 3600: -13.36\n",
      "adversarial loss at step 3600: 16.78\n",
      "discriminator loss at step 3800: 38.72\n",
      "adversarial loss at step 3800: -120.76\n",
      "discriminator loss at step 4000: -9.79\n",
      "adversarial loss at step 4000: 35.74\n",
      "discriminator loss at step 4200: -5.96\n",
      "adversarial loss at step 4200: -54.82\n",
      "discriminator loss at step 4400: -5.75\n",
      "adversarial loss at step 4400: -52.77\n",
      "discriminator loss at step 4600: -6.99\n",
      "adversarial loss at step 4600: -10.43\n",
      "discriminator loss at step 4800: -17.24\n",
      "adversarial loss at step 4800: -302.60\n",
      "discriminator loss at step 5000: 4.87\n",
      "adversarial loss at step 5000: -243.80\n",
      "discriminator loss at step 5200: -13.13\n",
      "adversarial loss at step 5200: 235.10\n",
      "discriminator loss at step 5400: -23.67\n",
      "adversarial loss at step 5400: -46.46\n",
      "discriminator loss at step 5600: -11.09\n",
      "adversarial loss at step 5600: 93.81\n",
      "discriminator loss at step 5800: -12.96\n",
      "adversarial loss at step 5800: -63.44\n",
      "discriminator loss at step 6000: -6.66\n",
      "adversarial loss at step 6000: -45.65\n",
      "discriminator loss at step 6200: -8.98\n",
      "adversarial loss at step 6200: -11.31\n",
      "./face_dis2\\ckpt-4\n",
      "\n",
      "Start epoch 4\n",
      "discriminator loss at step 0: -20.75\n",
      "adversarial loss at step 0: 202.97\n",
      "discriminator loss at step 200: -10.07\n",
      "adversarial loss at step 200: 228.97\n",
      "discriminator loss at step 400: -21.65\n",
      "adversarial loss at step 400: 206.50\n",
      "discriminator loss at step 600: 0.20\n",
      "adversarial loss at step 600: 239.54\n",
      "discriminator loss at step 800: 11.16\n",
      "adversarial loss at step 800: -124.02\n",
      "discriminator loss at step 1000: -6.30\n",
      "adversarial loss at step 1000: 99.18\n",
      "discriminator loss at step 1200: -9.06\n",
      "adversarial loss at step 1200: -60.07\n",
      "discriminator loss at step 1400: -6.68\n",
      "adversarial loss at step 1400: 3.77\n",
      "discriminator loss at step 1600: -11.28\n",
      "adversarial loss at step 1600: -5.60\n",
      "discriminator loss at step 1800: -10.11\n",
      "adversarial loss at step 1800: 7.95\n",
      "discriminator loss at step 2000: -7.95\n",
      "adversarial loss at step 2000: -56.05\n",
      "discriminator loss at step 2200: -11.13\n",
      "adversarial loss at step 2200: -32.38\n",
      "discriminator loss at step 2400: -14.32\n",
      "adversarial loss at step 2400: -44.74\n",
      "discriminator loss at step 2600: -6.92\n",
      "adversarial loss at step 2600: -219.49\n",
      "discriminator loss at step 2800: -7.74\n",
      "adversarial loss at step 2800: 162.82\n",
      "discriminator loss at step 3000: -18.74\n",
      "adversarial loss at step 3000: 211.06\n",
      "discriminator loss at step 3200: -16.66\n",
      "adversarial loss at step 3200: 73.87\n",
      "discriminator loss at step 3400: -16.90\n",
      "adversarial loss at step 3400: -233.27\n",
      "discriminator loss at step 3600: 7.67\n",
      "adversarial loss at step 3600: -92.57\n",
      "discriminator loss at step 3800: -21.21\n",
      "adversarial loss at step 3800: 29.70\n",
      "discriminator loss at step 4000: -10.66\n",
      "adversarial loss at step 4000: 161.94\n",
      "discriminator loss at step 4200: -16.19\n",
      "adversarial loss at step 4200: -131.17\n",
      "discriminator loss at step 4400: -0.57\n",
      "adversarial loss at step 4400: 10.82\n",
      "discriminator loss at step 4600: -0.68\n",
      "adversarial loss at step 4600: 108.77\n",
      "discriminator loss at step 4800: 3.61\n",
      "adversarial loss at step 4800: 129.66\n",
      "discriminator loss at step 5000: -7.85\n",
      "adversarial loss at step 5000: -10.50\n",
      "discriminator loss at step 5200: -6.55\n",
      "adversarial loss at step 5200: 67.25\n",
      "discriminator loss at step 5400: -22.36\n",
      "adversarial loss at step 5400: -99.67\n",
      "discriminator loss at step 5600: -2.10\n",
      "adversarial loss at step 5600: -89.63\n",
      "discriminator loss at step 5800: -11.08\n",
      "adversarial loss at step 5800: 189.69\n",
      "discriminator loss at step 6000: -20.71\n",
      "adversarial loss at step 6000: -266.79\n",
      "discriminator loss at step 6200: -18.88\n",
      "adversarial loss at step 6200: -91.71\n",
      "./face_dis2\\ckpt-5\n",
      "\n",
      "Start epoch 5\n",
      "discriminator loss at step 0: -9.80\n",
      "adversarial loss at step 0: -178.48\n",
      "discriminator loss at step 200: -8.54\n",
      "adversarial loss at step 200: 66.87\n",
      "discriminator loss at step 400: -12.02\n",
      "adversarial loss at step 400: 205.50\n",
      "discriminator loss at step 600: 3.21\n",
      "adversarial loss at step 600: 17.65\n",
      "discriminator loss at step 800: -16.61\n",
      "adversarial loss at step 800: -425.73\n",
      "discriminator loss at step 1000: -12.54\n",
      "adversarial loss at step 1000: 78.87\n",
      "discriminator loss at step 1200: -10.72\n",
      "adversarial loss at step 1200: -57.55\n",
      "discriminator loss at step 1400: -19.10\n",
      "adversarial loss at step 1400: -88.29\n",
      "discriminator loss at step 1600: -8.16\n",
      "adversarial loss at step 1600: 7.84\n",
      "discriminator loss at step 1800: -23.39\n",
      "adversarial loss at step 1800: -19.45\n",
      "discriminator loss at step 2000: -0.54\n",
      "adversarial loss at step 2000: 16.95\n",
      "discriminator loss at step 2200: -21.90\n",
      "adversarial loss at step 2200: -395.97\n",
      "discriminator loss at step 2400: -6.92\n",
      "adversarial loss at step 2400: -23.26\n",
      "discriminator loss at step 2600: -11.79\n",
      "adversarial loss at step 2600: -142.92\n",
      "discriminator loss at step 2800: 8.88\n",
      "adversarial loss at step 2800: -91.34\n",
      "discriminator loss at step 3000: -16.50\n",
      "adversarial loss at step 3000: -143.04\n",
      "discriminator loss at step 3200: -19.82\n",
      "adversarial loss at step 3200: 155.01\n",
      "discriminator loss at step 3400: -16.63\n",
      "adversarial loss at step 3400: -104.65\n",
      "discriminator loss at step 3600: -14.64\n",
      "adversarial loss at step 3600: -164.80\n",
      "discriminator loss at step 3800: -10.85\n",
      "adversarial loss at step 3800: 136.04\n",
      "discriminator loss at step 4000: -15.36\n",
      "adversarial loss at step 4000: 153.72\n",
      "discriminator loss at step 4200: -2.84\n",
      "adversarial loss at step 4200: 36.90\n",
      "discriminator loss at step 4400: -5.90\n",
      "adversarial loss at step 4400: 126.35\n",
      "discriminator loss at step 4600: -16.27\n",
      "adversarial loss at step 4600: -74.55\n",
      "discriminator loss at step 4800: -7.68\n",
      "adversarial loss at step 4800: 290.14\n",
      "discriminator loss at step 5000: -11.83\n",
      "adversarial loss at step 5000: 79.17\n",
      "discriminator loss at step 5200: -15.90\n",
      "adversarial loss at step 5200: -27.69\n",
      "discriminator loss at step 5400: -20.03\n",
      "adversarial loss at step 5400: -256.62\n",
      "discriminator loss at step 5600: -12.08\n",
      "adversarial loss at step 5600: 326.11\n",
      "discriminator loss at step 5800: -22.15\n",
      "adversarial loss at step 5800: 96.47\n",
      "discriminator loss at step 6000: -0.80\n",
      "adversarial loss at step 6000: 54.58\n",
      "discriminator loss at step 6200: -17.26\n",
      "adversarial loss at step 6200: 427.08\n",
      "./face_dis2\\ckpt-6\n",
      "\n",
      "Start epoch 6\n",
      "discriminator loss at step 0: -15.02\n",
      "adversarial loss at step 0: -39.48\n",
      "discriminator loss at step 200: -7.84\n",
      "adversarial loss at step 200: -84.25\n",
      "discriminator loss at step 400: -16.04\n",
      "adversarial loss at step 400: 53.24\n",
      "discriminator loss at step 600: 1.60\n",
      "adversarial loss at step 600: 225.66\n",
      "discriminator loss at step 800: -4.48\n",
      "adversarial loss at step 800: 172.59\n",
      "discriminator loss at step 1000: -11.82\n",
      "adversarial loss at step 1000: 203.42\n",
      "discriminator loss at step 1200: -18.77\n",
      "adversarial loss at step 1200: -19.90\n",
      "discriminator loss at step 1400: -21.91\n",
      "adversarial loss at step 1400: 56.31\n",
      "discriminator loss at step 1600: -8.01\n",
      "adversarial loss at step 1600: -354.46\n",
      "discriminator loss at step 1800: -16.02\n",
      "adversarial loss at step 1800: -38.25\n",
      "discriminator loss at step 2000: -7.86\n",
      "adversarial loss at step 2000: 5.76\n",
      "discriminator loss at step 2200: -16.48\n",
      "adversarial loss at step 2200: 304.42\n",
      "discriminator loss at step 2400: -9.52\n",
      "adversarial loss at step 2400: -219.95\n",
      "discriminator loss at step 2600: -6.96\n",
      "adversarial loss at step 2600: 217.83\n",
      "discriminator loss at step 2800: -17.36\n",
      "adversarial loss at step 2800: 20.95\n",
      "discriminator loss at step 3000: 1.69\n",
      "adversarial loss at step 3000: 88.58\n",
      "discriminator loss at step 3200: -4.54\n",
      "adversarial loss at step 3200: -168.16\n",
      "discriminator loss at step 3400: -3.05\n",
      "adversarial loss at step 3400: 214.07\n",
      "discriminator loss at step 3600: -20.65\n",
      "adversarial loss at step 3600: -111.60\n",
      "discriminator loss at step 3800: -12.04\n",
      "adversarial loss at step 3800: -183.54\n",
      "discriminator loss at step 4000: -17.34\n",
      "adversarial loss at step 4000: 73.19\n",
      "discriminator loss at step 4200: -14.22\n",
      "adversarial loss at step 4200: -208.85\n",
      "discriminator loss at step 4400: -21.21\n",
      "adversarial loss at step 4400: 543.50\n",
      "discriminator loss at step 4600: -8.52\n",
      "adversarial loss at step 4600: -136.12\n",
      "discriminator loss at step 4800: -13.63\n",
      "adversarial loss at step 4800: 166.36\n",
      "discriminator loss at step 5000: -1.19\n",
      "adversarial loss at step 5000: -27.82\n",
      "discriminator loss at step 5200: -16.84\n",
      "adversarial loss at step 5200: -316.26\n",
      "discriminator loss at step 5400: -14.14\n",
      "adversarial loss at step 5400: 300.51\n",
      "discriminator loss at step 5600: -2.82\n",
      "adversarial loss at step 5600: 6.90\n",
      "discriminator loss at step 5800: -13.55\n",
      "adversarial loss at step 5800: 54.68\n",
      "discriminator loss at step 6000: -5.31\n",
      "adversarial loss at step 6000: 33.27\n",
      "discriminator loss at step 6200: -4.64\n",
      "adversarial loss at step 6200: 102.19\n",
      "./face_dis2\\ckpt-7\n",
      "\n",
      "Start epoch 7\n",
      "discriminator loss at step 0: -31.47\n",
      "adversarial loss at step 0: 420.02\n",
      "discriminator loss at step 200: -17.08\n",
      "adversarial loss at step 200: 0.03\n",
      "discriminator loss at step 400: 1.97\n",
      "adversarial loss at step 400: 112.17\n",
      "discriminator loss at step 600: -17.06\n",
      "adversarial loss at step 600: 292.07\n",
      "discriminator loss at step 800: -15.18\n",
      "adversarial loss at step 800: -21.72\n",
      "discriminator loss at step 1000: -12.81\n",
      "adversarial loss at step 1000: 38.63\n",
      "discriminator loss at step 1200: -18.81\n",
      "adversarial loss at step 1200: 199.80\n",
      "discriminator loss at step 1400: -14.38\n",
      "adversarial loss at step 1400: 294.47\n",
      "discriminator loss at step 1600: -15.32\n",
      "adversarial loss at step 1600: -91.88\n",
      "discriminator loss at step 1800: -7.53\n",
      "adversarial loss at step 1800: -19.96\n",
      "discriminator loss at step 2000: -6.99\n",
      "adversarial loss at step 2000: 81.96\n",
      "discriminator loss at step 2200: 2.56\n",
      "adversarial loss at step 2200: 79.27\n",
      "discriminator loss at step 2400: -13.24\n",
      "adversarial loss at step 2400: 130.55\n",
      "discriminator loss at step 2600: -5.71\n",
      "adversarial loss at step 2600: 138.91\n",
      "discriminator loss at step 2800: -7.94\n",
      "adversarial loss at step 2800: -85.13\n",
      "discriminator loss at step 3000: 4.55\n",
      "adversarial loss at step 3000: -64.29\n",
      "discriminator loss at step 3200: -9.22\n",
      "adversarial loss at step 3200: 3.57\n",
      "discriminator loss at step 3400: -4.24\n",
      "adversarial loss at step 3400: 15.97\n",
      "discriminator loss at step 3600: -9.07\n",
      "adversarial loss at step 3600: 144.02\n",
      "discriminator loss at step 3800: -6.48\n",
      "adversarial loss at step 3800: -168.00\n",
      "discriminator loss at step 4000: -5.48\n",
      "adversarial loss at step 4000: -26.43\n",
      "discriminator loss at step 4200: -16.40\n",
      "adversarial loss at step 4200: 584.79\n",
      "discriminator loss at step 4400: -17.43\n",
      "adversarial loss at step 4400: 191.63\n",
      "discriminator loss at step 4600: -7.99\n",
      "adversarial loss at step 4600: -110.23\n",
      "discriminator loss at step 4800: -21.45\n",
      "adversarial loss at step 4800: -223.97\n",
      "discriminator loss at step 5000: -19.79\n",
      "adversarial loss at step 5000: 67.62\n",
      "discriminator loss at step 5200: -7.43\n",
      "adversarial loss at step 5200: 197.44\n",
      "discriminator loss at step 5400: -22.09\n",
      "adversarial loss at step 5400: 179.98\n",
      "discriminator loss at step 5600: -6.36\n",
      "adversarial loss at step 5600: 215.79\n",
      "discriminator loss at step 5800: -11.68\n",
      "adversarial loss at step 5800: 96.52\n",
      "discriminator loss at step 6000: 1.98\n",
      "adversarial loss at step 6000: 52.50\n",
      "discriminator loss at step 6200: -2.72\n",
      "adversarial loss at step 6200: -7.67\n",
      "./face_dis2\\ckpt-8\n",
      "\n",
      "Start epoch 8\n",
      "discriminator loss at step 0: -11.81\n",
      "adversarial loss at step 0: 232.42\n",
      "discriminator loss at step 200: -21.66\n",
      "adversarial loss at step 200: -61.71\n",
      "discriminator loss at step 400: -5.32\n",
      "adversarial loss at step 400: 4.51\n",
      "discriminator loss at step 600: -1.50\n",
      "adversarial loss at step 600: 25.87\n",
      "discriminator loss at step 800: -9.56\n",
      "adversarial loss at step 800: 248.15\n",
      "discriminator loss at step 1000: -13.57\n",
      "adversarial loss at step 1000: 42.90\n",
      "discriminator loss at step 1200: -17.14\n",
      "adversarial loss at step 1200: 109.61\n",
      "discriminator loss at step 1400: -5.02\n",
      "adversarial loss at step 1400: 35.40\n",
      "discriminator loss at step 1600: -19.13\n",
      "adversarial loss at step 1600: 84.15\n",
      "discriminator loss at step 1800: -1.16\n",
      "adversarial loss at step 1800: 39.44\n",
      "discriminator loss at step 2000: -10.60\n",
      "adversarial loss at step 2000: 456.44\n",
      "discriminator loss at step 2200: -23.01\n",
      "adversarial loss at step 2200: 216.94\n",
      "discriminator loss at step 2400: -8.33\n",
      "adversarial loss at step 2400: -99.21\n",
      "discriminator loss at step 2600: -4.78\n",
      "adversarial loss at step 2600: 15.32\n",
      "discriminator loss at step 2800: -8.29\n",
      "adversarial loss at step 2800: -118.05\n",
      "discriminator loss at step 3000: -20.03\n",
      "adversarial loss at step 3000: 98.63\n",
      "discriminator loss at step 3200: 6.64\n",
      "adversarial loss at step 3200: -53.10\n",
      "discriminator loss at step 3400: -21.92\n",
      "adversarial loss at step 3400: 217.95\n",
      "discriminator loss at step 3600: -14.32\n",
      "adversarial loss at step 3600: -255.63\n",
      "discriminator loss at step 3800: -17.12\n",
      "adversarial loss at step 3800: -139.00\n",
      "discriminator loss at step 4000: -2.67\n",
      "adversarial loss at step 4000: 357.02\n",
      "discriminator loss at step 4200: -11.31\n",
      "adversarial loss at step 4200: 109.35\n",
      "discriminator loss at step 4400: -8.29\n",
      "adversarial loss at step 4400: -44.33\n",
      "discriminator loss at step 4600: -17.92\n",
      "adversarial loss at step 4600: -39.76\n",
      "discriminator loss at step 4800: -23.48\n",
      "adversarial loss at step 4800: 71.56\n",
      "discriminator loss at step 5000: -15.74\n",
      "adversarial loss at step 5000: 184.26\n",
      "discriminator loss at step 5200: 22.75\n",
      "adversarial loss at step 5200: 294.54\n",
      "discriminator loss at step 5400: -21.01\n",
      "adversarial loss at step 5400: 94.11\n",
      "discriminator loss at step 5600: 1.26\n",
      "adversarial loss at step 5600: 226.09\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0e101e912869>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# rand_label1=keras.utils.to_categorical(np.random.randint(10,size=(batch_size,1)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# tf.random.set_seed(int(time.time()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerated_images\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m# print(real_images.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Logging.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf2.0\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 2000       # In practice you need at least 20 epochs to generate nice digits.\n",
    "save_dir = \"./\"\n",
    "dataset1=dataset1.shuffle(1024)\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart epoch\", epoch)\n",
    "    for step, real_images in enumerate(dataset1):\n",
    "        # break\n",
    "        # Train the discriminator & generator on one batch of real images.\n",
    "        # rand_label1=keras.utils.to_categorical(np.random.randint(10,size=(batch_size,1)))\n",
    "        # tf.random.set_seed(int(time.time()))\n",
    "        d_loss, g_loss, generated_images= train_step(real_images)\n",
    "        # print(real_images.shape)\n",
    "        # Logging.\n",
    "        if step % 200 == 0:\n",
    "            # Print metrics\n",
    "            print(\"discriminator loss at step %d: %.2f\" % (step, d_loss))\n",
    "            print(\"adversarial loss at step %d: %.2f\" % (step, g_loss))\n",
    "\n",
    "            # Save one generated image\n",
    "            img = tf.keras.preprocessing.image.array_to_img(\n",
    "                generated_images[0] * 255.0, scale=False\n",
    "            )\n",
    "            img.save(os.path.join(save_dir, \"generated_img_gan2_\" + str(step) + \".png\"))\n",
    "        #     # print(np.argmax(rand_label1,axis=-1)[0])\n",
    "        # # To limit execution time we stop after 10 steps.\n",
    "        # # Remove the lines below to actually train the model!\n",
    "        # # if step > 10:\n",
    "        # #     break\n",
    "    print(manager.save())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}